{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b68391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('C:/Users/USER/Desktop/Data_Practices/Disaster_Tweeter/train.csv')\n",
    "test_df = pd.read_csv('C:/Users/USER/Desktop/Data_Practices/Disaster_Tweeter/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d10425",
   "metadata": {},
   "source": [
    "## 훈련 데이터 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e581b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68ca7ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'ablaze', 'accident', 'aftershock', 'airplane%20accident',\n",
       "       'ambulance', 'annihilated', 'annihilation', 'apocalypse',\n",
       "       'armageddon'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 키워드에 대한 변수 확인\n",
    "train_df.keyword.unique()[:10]\n",
    "# 키워드에 따라 어떤 정보가 주어져있을 가능성이 매우 높다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b17f1d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target에 대한 정보 확인\n",
    "train_df.target.value_counts()\n",
    "# 재난 트윗 3271건, 비재난 트윗 4342건 발생\n",
    "# 주어진 데이터는 크게 불균형이 발생하지 않았다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666663b4",
   "metadata": {},
   "source": [
    "## 문자열 정제\n",
    "다음과 같은 정제가 필요할 것이다.  \n",
    "1. 영어이므로 대소문자를 통일한다.\n",
    "2. 특수기호 등을 없애줘야 한다.  \n",
    "3. 의미 없는 문법에 의한 단어들(stopwords)를 제거한다.\n",
    "\n",
    "이 중 stop_words의 경우에는, tf-idf vectorizer 과정에서 지정가능하므로\n",
    "나머지 과정에 대해 처리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dd97235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text) # 괄호 제거\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text) # 링크 제거\n",
    "    text = re.sub('<.*?>+', '', text) \n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) #구두점 제거\n",
    "    text = re.sub('\\n', '', text) # \n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebdf91d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.text = train_df.text.apply(lambda s : clean_text(s))\n",
    "test_df.text = test_df.text.apply(lambda s : clean_text(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18f75e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive wildfires evacuation orders in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ariaahrary thetawniest the out of control wild...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>s of volcano hawaii</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>police investigating after an ebike collided w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the latest more homes razed by northern califo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     our deeds are the reason of this earthquake ma...       1  \n",
       "1                 forest fire near la ronge sask canada       1  \n",
       "2     all residents asked to shelter in place are be...       1  \n",
       "3      people receive wildfires evacuation orders in...       1  \n",
       "4     just got sent this photo from ruby alaska as s...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  two giant cranes holding a bridge collapse int...       1  \n",
       "7609  ariaahrary thetawniest the out of control wild...       1  \n",
       "7610                               s of volcano hawaii        1  \n",
       "7611  police investigating after an ebike collided w...       1  \n",
       "7612  the latest more homes razed by northern califo...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f00d0577",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.fillna('.')\n",
    "test_df = test_df.fillna('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26ce05d",
   "metadata": {},
   "source": [
    "## Vectorizer 적용 후의 간단한 분류 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7732a0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(stop_words = 'english')\n",
    "train_vec = tfidf.fit_transform(train_df.text)\n",
    "test_vec = tfidf.transform(test_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25022c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vec.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9ca6d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.59398496 0.58263773 0.6460251  0.60493827 0.73635665]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf_nb = MultinomialNB()\n",
    "scores = cross_val_score(clf_nb, train_vec, train_df['target'], cv = 5,\n",
    "                        scoring = 'f1')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f786400",
   "metadata": {},
   "source": [
    "## 모델 개선 시도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaee6f78",
   "metadata": {},
   "source": [
    "다음에 대해 고려하고 모델을 적용해보자.  \n",
    "- 단어의 기본형만 사용할 수 없을 것인가?\n",
    "- 다른 벡터화는 존재하지 않는가?\n",
    "- 다른 정보를 사용할 수는 없을 것인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e606b8",
   "metadata": {},
   "source": [
    "### lemmatizer(표제어 추출)\n",
    "이 과정을 통해 단어를 표제어 기준의 형태로 변경해볼 수 있다.  \n",
    "하지만 이를 위해서는 위의 형태로 진행하는 것이 아닌 토큰화를 별도로 진행해야 할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44c68933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1008ac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text):\n",
    "    tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    lemmatize_result = \" \".join(lemmatizer.lemmatize(token) for token in tokens)\n",
    "    \n",
    "    return lemmatize_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55de6715",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.text = train_df.text.apply(lambda s : text_preprocess(s))\n",
    "test_df.text = test_df.text.apply(lambda s : text_preprocess(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9cd4fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>our deed are the reason of this earthquake may...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>all resident asked to shelter in place are bei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>people receive wildfire evacuation order in ca...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>just got sent this photo from ruby alaska a sm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>two giant crane holding a bridge collapse into...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>ariaahrary thetawniest the out of control wild...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>s of volcano hawaii</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>police investigating after an ebike collided w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>the latest more home razed by northern califor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1       .        .   \n",
       "1         4       .        .   \n",
       "2         5       .        .   \n",
       "3         6       .        .   \n",
       "4         7       .        .   \n",
       "...     ...     ...      ...   \n",
       "7608  10869       .        .   \n",
       "7609  10870       .        .   \n",
       "7610  10871       .        .   \n",
       "7611  10872       .        .   \n",
       "7612  10873       .        .   \n",
       "\n",
       "                                                   text  target  \n",
       "0     our deed are the reason of this earthquake may...       1  \n",
       "1                 forest fire near la ronge sask canada       1  \n",
       "2     all resident asked to shelter in place are bei...       1  \n",
       "3     people receive wildfire evacuation order in ca...       1  \n",
       "4     just got sent this photo from ruby alaska a sm...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  two giant crane holding a bridge collapse into...       1  \n",
       "7609  ariaahrary thetawniest the out of control wild...       1  \n",
       "7610                                s of volcano hawaii       1  \n",
       "7611  police investigating after an ebike collided w...       1  \n",
       "7612  the latest more home razed by northern califor...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aad4e35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.59128823 0.60855263 0.64585045 0.61672474 0.74294432]\n"
     ]
    }
   ],
   "source": [
    "# 추가 전처리 후 동일 모델로 재평가\n",
    "tfidf = TfidfVectorizer(stop_words = 'english')\n",
    "train_vec = tfidf.fit_transform(train_df.text)\n",
    "test_vec = tfidf.transform(test_df.text)\n",
    "\n",
    "clf_nb = MultinomialNB()\n",
    "scores = cross_val_score(clf_nb, train_vec, train_df['target'], cv = 5,\n",
    "                        scoring = 'f1')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74298551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62966031 0.60770328 0.65671642 0.66010598 0.73203492]\n"
     ]
    }
   ],
   "source": [
    "# 다른 vectorizer 활용\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cnt = CountVectorizer(stop_words = 'english')\n",
    "train_vec = cnt.fit_transform(train_df.text)\n",
    "test_vec = cnt.transform(test_df.text)\n",
    "\n",
    "clf_nb = MultinomialNB()\n",
    "scores = cross_val_score(clf_nb, train_vec, train_df['target'], cv = 5,\n",
    "                        scoring = 'f1')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ddf81f",
   "metadata": {},
   "source": [
    "count vectorzier에서 더 좋은 성능을 보이고 있는데, 이는 재난 문자에서는 공통적인 키워드가 들어가 있을 것이기에, 그 공통적인 키워드의 빈출을 살리기 위해서는 문서별 특이어를 잡아주는 TF-IDF보다는 공통어에 치중된 CountVectorzier의 성능이 더 좋을 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b63342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 확인을 위한 작업\n",
    "def submission(submission_file_path,model,test_vectors):\n",
    "    sample_submission = pd.read_csv(submission_file_path)\n",
    "    sample_submission[\"target\"] = model.predict(test_vectors)\n",
    "    sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20633015",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file_path = 'C:/Users/USER/Desktop/Data_Practices/Disaster_Tweeter/sample_submission.csv'\n",
    "test_vectors=test_vec\n",
    "model = clf_nb\n",
    "model.fit(train_vec, train_df['target'])\n",
    "submission(submission_file_path,clf_nb,test_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc18d3f7",
   "metadata": {},
   "source": [
    "위의 모델은 0.79436이라는 결과를 얻었다. 이제 조금 더 좋은 결과를 얻기 위해 개선 작업을 진행하고자 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734346d2",
   "metadata": {},
   "source": [
    "### 추가 변수 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0431a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>48</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>bbcmtd wholesale market ablaze</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>49</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Est. September 2012 - Bristol</td>\n",
       "      <td>we always try to bring the heavy metal rt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>50</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>africanbaze breaking newsnigeria flag set abla...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>52</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>cry out for more set me ablaze</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>53</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>London, UK</td>\n",
       "      <td>on plus side look at the sky last night it wa ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7578</th>\n",
       "      <td>10830</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>.</td>\n",
       "      <td>cameronhacker and i wrecked you both</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7579</th>\n",
       "      <td>10831</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Vancouver, Canada</td>\n",
       "      <td>three day off from work and theyve pretty much...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7580</th>\n",
       "      <td>10832</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>London</td>\n",
       "      <td>fx forex trading cramer igers word that wrecke...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7581</th>\n",
       "      <td>10833</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Lincoln</td>\n",
       "      <td>engineshed great atmosphere at the british lio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7582</th>\n",
       "      <td>10834</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>.</td>\n",
       "      <td>cramer igers word that wrecked disney stock cnbc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7552 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  keyword                       location  \\\n",
       "31       48   ablaze                     Birmingham   \n",
       "32       49   ablaze  Est. September 2012 - Bristol   \n",
       "33       50   ablaze                         AFRICA   \n",
       "34       52   ablaze               Philadelphia, PA   \n",
       "35       53   ablaze                     London, UK   \n",
       "...     ...      ...                            ...   \n",
       "7578  10830  wrecked                              .   \n",
       "7579  10831  wrecked              Vancouver, Canada   \n",
       "7580  10832  wrecked                        London    \n",
       "7581  10833  wrecked                        Lincoln   \n",
       "7582  10834  wrecked                              .   \n",
       "\n",
       "                                                   text  target  \n",
       "31                       bbcmtd wholesale market ablaze       1  \n",
       "32            we always try to bring the heavy metal rt       0  \n",
       "33    africanbaze breaking newsnigeria flag set abla...       1  \n",
       "34                       cry out for more set me ablaze       0  \n",
       "35    on plus side look at the sky last night it wa ...       0  \n",
       "...                                                 ...     ...  \n",
       "7578               cameronhacker and i wrecked you both       0  \n",
       "7579  three day off from work and theyve pretty much...       0  \n",
       "7580  fx forex trading cramer igers word that wrecke...       0  \n",
       "7581  engineshed great atmosphere at the british lio...       0  \n",
       "7582   cramer igers word that wrecked disney stock cnbc       0  \n",
       "\n",
       "[7552 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keyword가 있는 경우, 이미 keyword가 text에 포함되어 있음을 확인해 볼 수 있다.\n",
    "train_df.loc[train_df.keyword != '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1fecb0",
   "metadata": {},
   "source": [
    "### 다른 Word-Embedding : Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4117c5",
   "metadata": {},
   "source": [
    "위의 두 모델은 단일 단어에 대해서만 고려하는 모델이다. 단어의 문맥을 고려하기 위해서는 Word2Vec이 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35c5527e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\user\\anaconda3\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (0.29.23)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (1.20.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (1.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995b29aa",
   "metadata": {},
   "source": [
    "Word2Vec을 위해서는 전체 코퍼스를 만들어야 하며,\n",
    "또한 이를 위해서는 앞에처럼 CountVectorizer 단계에서 stopwords를 지정하는 것이 아닌, 사전에 이를 지정해서 제외해야 하고, 이를 위해서는 토큰화된 상황으로 시작해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e52ee4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0a02ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def text_preprocess(text):\n",
    "    tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    lemmatizes = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    words = [w for w in lemmatizes if w not in stopwords.words('english')]    \n",
    "    combined_word = ' '.join(w for w in words)\n",
    "    \n",
    "    return combined_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7e6be7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.text = train_df.text.apply(lambda s : text_preprocess(s))\n",
    "test_df.text = test_df.text.apply(lambda s : text_preprocess(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bb96b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus 형성, 이 때 train, test 모두 해야 사이즈가 맞는다.\n",
    "corpus = pd.concat([train_df, test_df]).text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "318bb6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3b2de10",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_token = [nltk.word_tokenize(word) for word in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94522091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "word_vector_model = Word2Vec(sentences = corpus_token, vector_size = 300, \n",
    "                             window = 5, min_count = 5, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af96f96e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('please', 0.9871126413345337),\n",
       " ('bush', 0.9855107069015503),\n",
       " ('alarm', 0.9790080189704895),\n",
       " ('could', 0.9783087968826294),\n",
       " ('drought', 0.9774770140647888),\n",
       " ('huge', 0.9754066467285156),\n",
       " ('danger', 0.9749287962913513),\n",
       " ('rage', 0.9747996926307678),\n",
       " ('evacuate', 0.9747645258903503),\n",
       " ('acre', 0.9740025401115417)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector_model.wv.most_similar('help')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1105d53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wild', 0.9464491605758667),\n",
       " ('forest', 0.9394634962081909),\n",
       " ('annihilation', 0.9214696288108826),\n",
       " ('river', 0.9198334217071533),\n",
       " ('salt', 0.9186375141143799),\n",
       " ('tonto', 0.9073498249053955),\n",
       " ('national', 0.9071736335754395),\n",
       " ('service', 0.9027889370918274),\n",
       " ('park', 0.8991431593894958),\n",
       " ('stop', 0.8965114951133728)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector_model.wv.most_similar('fire')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844bac33",
   "metadata": {},
   "source": [
    "Word2Vec를 통해서는 유사한 단어를 추출해준다. 다시 말해, 단어 별로 임베딩 벡터가 주어지게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a0aa81",
   "metadata": {},
   "source": [
    "## Word2Vec을 활용한 모델(RNN 비사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08973b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>two giant crane holding bridge collapse nearby...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>ariaahrary thetawniest control wild fire calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>volcano hawaii</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>police investigating ebike collided car little...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>latest home razed northern california wildfire...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1       .        .   \n",
       "1         4       .        .   \n",
       "2         5       .        .   \n",
       "3         6       .        .   \n",
       "4         7       .        .   \n",
       "...     ...     ...      ...   \n",
       "7608  10869       .        .   \n",
       "7609  10870       .        .   \n",
       "7610  10871       .        .   \n",
       "7611  10872       .        .   \n",
       "7612  10873       .        .   \n",
       "\n",
       "                                                   text  target  \n",
       "0            deed reason earthquake may allah forgive u       1  \n",
       "1                 forest fire near la ronge sask canada       1  \n",
       "2     resident asked shelter place notified officer ...       1  \n",
       "3     people receive wildfire evacuation order calif...       1  \n",
       "4     got sent photo ruby alaska smoke wildfire pour...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  two giant crane holding bridge collapse nearby...       1  \n",
       "7609  ariaahrary thetawniest control wild fire calif...       1  \n",
       "7610                                     volcano hawaii       1  \n",
       "7611  police investigating ebike collided car little...       1  \n",
       "7612  latest home razed northern california wildfire...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7521d0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_tokenize(text):\n",
    "    tokenizer = nltk.tokenize.WhitespaceTokenizer()   \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09177661",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.text = train_df.text.apply(lambda s : text_tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c9906d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disaster과의 유사성\n",
    "def get_average_word2vec(tokens):\n",
    "    sim_list = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            # 앞에까지의 문맥을 바탕으로 한 재난 연관도와, 이어옴\n",
    "            sim_list.append(word_vector_model.wv.similarity(token, 'disaster'))\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if len(sim_list) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        if len(sim_list) <= 3:\n",
    "            return sum(sim_list)/len(sim_list)\n",
    "        else:\n",
    "            sorted_sim = sorted(sim_list, reverse = True)[:3]\n",
    "            return sum(sorted_sim)/3\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f6c41bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['avg_word2vec'] = train_df.text.apply(lambda s : get_average_word2vec(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "84cb18aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>avg_word2vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>[deed, reason, earthquake, may, allah, forgive...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.542853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.493892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>[resident, asked, shelter, place, notified, of...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.541197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>[people, receive, wildfire, evacuation, order,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.529993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.534110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>[two, giant, crane, holding, bridge, collapse,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.623134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>[ariaahrary, thetawniest, control, wild, fire,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.613071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>[volcano, hawaii]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.541911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>[police, investigating, ebike, collided, car, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.541644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>[latest, home, razed, northern, california, wi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.656234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1       .        .   \n",
       "1         4       .        .   \n",
       "2         5       .        .   \n",
       "3         6       .        .   \n",
       "4         7       .        .   \n",
       "...     ...     ...      ...   \n",
       "7608  10869       .        .   \n",
       "7609  10870       .        .   \n",
       "7610  10871       .        .   \n",
       "7611  10872       .        .   \n",
       "7612  10873       .        .   \n",
       "\n",
       "                                                   text  target  avg_word2vec  \n",
       "0     [deed, reason, earthquake, may, allah, forgive...       1      0.542853  \n",
       "1         [forest, fire, near, la, ronge, sask, canada]       1      0.493892  \n",
       "2     [resident, asked, shelter, place, notified, of...       1      0.541197  \n",
       "3     [people, receive, wildfire, evacuation, order,...       1      0.529993  \n",
       "4     [got, sent, photo, ruby, alaska, smoke, wildfi...       1      0.534110  \n",
       "...                                                 ...     ...           ...  \n",
       "7608  [two, giant, crane, holding, bridge, collapse,...       1      0.623134  \n",
       "7609  [ariaahrary, thetawniest, control, wild, fire,...       1      0.613071  \n",
       "7610                                  [volcano, hawaii]       1      0.541911  \n",
       "7611  [police, investigating, ebike, collided, car, ...       1      0.541644  \n",
       "7612  [latest, home, razed, northern, california, wi...       1      0.656234  \n",
       "\n",
       "[7613 rows x 6 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5cdfae80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>avg_word2vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3271.000000</td>\n",
       "      <td>3271.0</td>\n",
       "      <td>3271.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5661.608071</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.535080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3097.094809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.047595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3104.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.511653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5676.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.532719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8252.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.551172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10873.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.993304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  target  avg_word2vec\n",
       "count   3271.000000  3271.0   3271.000000\n",
       "mean    5661.608071     1.0      0.535080\n",
       "std     3097.094809     0.0      0.083847\n",
       "min        1.000000     1.0     -0.047595\n",
       "25%     3104.500000     1.0      0.511653\n",
       "50%     5676.000000     1.0      0.532719\n",
       "75%     8252.000000     1.0      0.551172\n",
       "max    10873.000000     1.0      0.993304"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[train_df.target == 1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7bb4d073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>avg_word2vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4342.000000</td>\n",
       "      <td>4342.0</td>\n",
       "      <td>4342.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5276.446338</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.518023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3157.206802</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.047595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2513.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.501860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5243.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8038.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.540616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10848.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.962967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  target  avg_word2vec\n",
       "count   4342.000000  4342.0   4342.000000\n",
       "mean    5276.446338     0.0      0.518023\n",
       "std     3157.206802     0.0      0.062311\n",
       "min       23.000000     0.0     -0.047595\n",
       "25%     2513.250000     0.0      0.501860\n",
       "50%     5243.500000     0.0      0.524978\n",
       "75%     8038.500000     0.0      0.540616\n",
       "max    10848.000000     0.0      0.962967"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[train_df.target == 0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f03e0bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['avg_word2vec'] = test_df.text.apply(lambda s : get_average_word2vec(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0d218987",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df[['avg_word2vec']]\n",
    "y_train = train_df['target']\n",
    "X_test = test_df[['avg_word2vec']]\n",
    "y_test = test_df['avg_word2vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "591a460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.3, random_state = 42,\n",
    "                                                     stratify = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d3d95ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "pred = lr.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "69e19dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1280,  916],\n",
       "       [  23,   65]], dtype=int64)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(pred, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c51b669",
   "metadata": {},
   "source": [
    "보다시피, 음성 모델에 대해 잘 분류하고 있지 못함을 알 수 있다.\n",
    "이는 단순히 주제와의 유사도만으로는 풀기에 좋지 않음을 확인했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "57c810a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>avg_word2vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>[deed, reason, earthquake, may, allah, forgive...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.542853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.493892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>[resident, asked, shelter, place, notified, of...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.541197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>[people, receive, wildfire, evacuation, order,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.529993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.534110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>[two, giant, crane, holding, bridge, collapse,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.623134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>[ariaahrary, thetawniest, control, wild, fire,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.613071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>[volcano, hawaii]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.541911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>[police, investigating, ebike, collided, car, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.541644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>[latest, home, razed, northern, california, wi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.656234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1       .        .   \n",
       "1         4       .        .   \n",
       "2         5       .        .   \n",
       "3         6       .        .   \n",
       "4         7       .        .   \n",
       "...     ...     ...      ...   \n",
       "7608  10869       .        .   \n",
       "7609  10870       .        .   \n",
       "7610  10871       .        .   \n",
       "7611  10872       .        .   \n",
       "7612  10873       .        .   \n",
       "\n",
       "                                                   text  target  avg_word2vec  \n",
       "0     [deed, reason, earthquake, may, allah, forgive...       1      0.542853  \n",
       "1         [forest, fire, near, la, ronge, sask, canada]       1      0.493892  \n",
       "2     [resident, asked, shelter, place, notified, of...       1      0.541197  \n",
       "3     [people, receive, wildfire, evacuation, order,...       1      0.529993  \n",
       "4     [got, sent, photo, ruby, alaska, smoke, wildfi...       1      0.534110  \n",
       "...                                                 ...     ...           ...  \n",
       "7608  [two, giant, crane, holding, bridge, collapse,...       1      0.623134  \n",
       "7609  [ariaahrary, thetawniest, control, wild, fire,...       1      0.613071  \n",
       "7610                                  [volcano, hawaii]       1      0.541911  \n",
       "7611  [police, investigating, ebike, collided, car, ...       1      0.541644  \n",
       "7612  [latest, home, razed, northern, california, wi...       1      0.656234  \n",
       "\n",
       "[7613 rows x 6 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "070e71b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 부분에 대한 개선이 필요.\n",
    "def return_avg_vec(tokens):\n",
    "    vec = [0.0] * 300\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            vec += word_vector_model.wv[token]\n",
    "            count += 1\n",
    "        except KeyError:\n",
    "            count += 0\n",
    "    if count == 0:\n",
    "        return [0.0] * 300\n",
    "    else:\n",
    "        return [elt/count for elt in vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "eeff173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['w2v_vector'] = train_df.text.apply(lambda s : return_avg_vec(s))\n",
    "test_df['w2v_vector'] = train_df.text.apply(lambda s : return_avg_vec(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cb0225a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df[['w2v_vector']]\n",
    "y_train = train_df['target']\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.2,\n",
    "                                                     random_state = 42, stratify = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6a175da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_df[['w2v_vector']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2c4364",
   "metadata": {},
   "source": [
    "### 신경망 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5bceb883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "tx_train = torch.Tensor(list(X_train.values))\n",
    "ty_train = torch.Tensor(y_train)\n",
    "tx_valid = torch.Tensor(list(X_valid.values))\n",
    "ty_valid = torch.Tensor(list(y_valid))\n",
    "tx_test = torch.Tensor(list(X_test.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8ef6d1",
   "metadata": {},
   "source": [
    "torch를 바탕으로 로지스틱 회귀 문제를 풀고자 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1bf52f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6090, 1, 300])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "bbe58c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6090])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ty_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "38a20999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3263, 1, 300])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6d327d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1523, 1, 300])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e1342364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1523])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ty_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "840bee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_train = tx_train.view(6090, 300)\n",
    "ty_train = ty_train.view(6090, 1)\n",
    "tx_test = tx_test.view(3263, 300)\n",
    "tx_valid = tx_valid.view(1523, 300)\n",
    "ty_valid = ty_valid.view(1523, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "bfac66b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e8a353a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 300차원을 1차원으로 줄여야 할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3debb959",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisasterClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(300, 60),\n",
    "            torch.nn.ReLU())\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(60, 10),\n",
    "            torch.nn.ReLU())\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(10, 1),\n",
    "            torch.nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a4d9b0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DisasterClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "178f3f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 0.509921 Accuracy 0.00%\n",
      "Epoch  100/1000 Cost: 0.509921 Accuracy 0.00%\n",
      "Epoch  200/1000 Cost: 0.509921 Accuracy 0.00%\n",
      "Epoch  300/1000 Cost: 0.509921 Accuracy 0.00%\n",
      "Epoch  400/1000 Cost: 0.509921 Accuracy 0.00%\n",
      "Epoch  500/1000 Cost: 0.509921 Accuracy 0.00%\n",
      "Epoch  600/1000 Cost: 0.509921 Accuracy 0.00%\n",
      "Epoch  700/1000 Cost: 0.509921 Accuracy 0.00%\n",
      "Epoch  800/1000 Cost: 0.509921 Accuracy 0.00%\n",
      "Epoch  900/1000 Cost: 0.509921 Accuracy 0.00%\n",
      "Epoch 1000/1000 Cost: 0.509921 Accuracy 0.00%\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-7)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = model(tx_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.binary_cross_entropy(hypothesis, ty_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        prediction = hypothesis >= torch.FloatTensor([0.35]) # 예측값이 0.5를 넘으면 True로 간주\n",
    "        correct_prediction = prediction.float() == y_train # 실제값과 일치하는 경우만 True로 간주\n",
    "        accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정확도를 계산\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format( # 각 에포크마다 정확도를 출력\n",
    "            epoch, nb_epochs, cost.item(), accuracy * 100,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba4a5d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
