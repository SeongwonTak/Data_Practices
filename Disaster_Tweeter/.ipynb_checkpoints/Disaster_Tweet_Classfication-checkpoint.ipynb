{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b68391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('C:/Users/USER/Desktop/Data_Practices/Disaster_Tweeter/train.csv')\n",
    "test_df = pd.read_csv('C:/Users/USER/Desktop/Data_Practices/Disaster_Tweeter/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d10425",
   "metadata": {},
   "source": [
    "## 훈련 데이터 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e581b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68ca7ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'ablaze', 'accident', 'aftershock', 'airplane%20accident',\n",
       "       'ambulance', 'annihilated', 'annihilation', 'apocalypse',\n",
       "       'armageddon'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 키워드에 대한 변수 확인\n",
    "train_df.keyword.unique()[:10]\n",
    "# 키워드에 따라 어떤 정보가 주어져있을 가능성이 매우 높다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b17f1d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target에 대한 정보 확인\n",
    "train_df.target.value_counts()\n",
    "# 재난 트윗 3271건, 비재난 트윗 4342건 발생\n",
    "# 주어진 데이터는 크게 불균형이 발생하지 않았다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666663b4",
   "metadata": {},
   "source": [
    "## 문자열 정제\n",
    "다음과 같은 정제가 필요할 것이다.  \n",
    "1. 영어이므로 대소문자를 통일한다.\n",
    "2. 특수기호 등을 없애줘야 한다.  \n",
    "3. 의미 없는 문법에 의한 단어들(stopwords)를 제거한다.\n",
    "\n",
    "이 중 stop_words의 경우에는, tf-idf vectorizer 과정에서 지정가능하므로\n",
    "나머지 과정에 대해 처리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dd97235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text) # 괄호 제거\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text) # 링크 제거\n",
    "    text = re.sub('<.*?>+', '', text) \n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) #구두점 제거\n",
    "    text = re.sub('\\n', '', text) # \n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebdf91d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.text = train_df.text.apply(lambda s : clean_text(s))\n",
    "test_df.text = test_df.text.apply(lambda s : clean_text(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18f75e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive wildfires evacuation orders in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ariaahrary thetawniest the out of control wild...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>s of volcano hawaii</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>police investigating after an ebike collided w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the latest more homes razed by northern califo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     our deeds are the reason of this earthquake ma...       1  \n",
       "1                 forest fire near la ronge sask canada       1  \n",
       "2     all residents asked to shelter in place are be...       1  \n",
       "3      people receive wildfires evacuation orders in...       1  \n",
       "4     just got sent this photo from ruby alaska as s...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  two giant cranes holding a bridge collapse int...       1  \n",
       "7609  ariaahrary thetawniest the out of control wild...       1  \n",
       "7610                               s of volcano hawaii        1  \n",
       "7611  police investigating after an ebike collided w...       1  \n",
       "7612  the latest more homes razed by northern califo...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f00d0577",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.fillna('.')\n",
    "test_df = test_df.fillna('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26ce05d",
   "metadata": {},
   "source": [
    "## Vectorizer 적용 후의 간단한 분류 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7732a0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(stop_words = 'english')\n",
    "train_vec = tfidf.fit_transform(train_df.text)\n",
    "test_vec = tfidf.transform(test_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25022c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vec.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9ca6d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.59398496 0.58263773 0.6460251  0.60493827 0.73635665]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf_nb = MultinomialNB()\n",
    "scores = cross_val_score(clf_nb, train_vec, train_df['target'], cv = 5,\n",
    "                        scoring = 'f1')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f786400",
   "metadata": {},
   "source": [
    "## 모델 개선 시도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaee6f78",
   "metadata": {},
   "source": [
    "다음에 대해 고려하고 모델을 적용해보자.  \n",
    "- 단어의 기본형만 사용할 수 없을 것인가?\n",
    "- 다른 벡터화는 존재하지 않는가?\n",
    "- 다른 정보를 사용할 수는 없을 것인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e606b8",
   "metadata": {},
   "source": [
    "### lemmatizer(표제어 추출)\n",
    "이 과정을 통해 단어를 표제어 기준의 형태로 변경해볼 수 있다.  \n",
    "하지만 이를 위해서는 위의 형태로 진행하는 것이 아닌 토큰화를 별도로 진행해야 할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44c68933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1008ac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text):\n",
    "    tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    lemmatize_result = \" \".join(lemmatizer.lemmatize(token) for token in tokens)\n",
    "    \n",
    "    return lemmatize_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55de6715",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.text = train_df.text.apply(lambda s : text_preprocess(s))\n",
    "test_df.text = test_df.text.apply(lambda s : text_preprocess(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9cd4fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>our deed are the reason of this earthquake may...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>all resident asked to shelter in place are bei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>people receive wildfire evacuation order in ca...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>just got sent this photo from ruby alaska a sm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>two giant crane holding a bridge collapse into...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>ariaahrary thetawniest the out of control wild...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>s of volcano hawaii</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>police investigating after an ebike collided w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>the latest more home razed by northern califor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1       .        .   \n",
       "1         4       .        .   \n",
       "2         5       .        .   \n",
       "3         6       .        .   \n",
       "4         7       .        .   \n",
       "...     ...     ...      ...   \n",
       "7608  10869       .        .   \n",
       "7609  10870       .        .   \n",
       "7610  10871       .        .   \n",
       "7611  10872       .        .   \n",
       "7612  10873       .        .   \n",
       "\n",
       "                                                   text  target  \n",
       "0     our deed are the reason of this earthquake may...       1  \n",
       "1                 forest fire near la ronge sask canada       1  \n",
       "2     all resident asked to shelter in place are bei...       1  \n",
       "3     people receive wildfire evacuation order in ca...       1  \n",
       "4     just got sent this photo from ruby alaska a sm...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  two giant crane holding a bridge collapse into...       1  \n",
       "7609  ariaahrary thetawniest the out of control wild...       1  \n",
       "7610                                s of volcano hawaii       1  \n",
       "7611  police investigating after an ebike collided w...       1  \n",
       "7612  the latest more home razed by northern califor...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aad4e35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.59128823 0.60855263 0.64585045 0.61672474 0.74294432]\n"
     ]
    }
   ],
   "source": [
    "# 추가 전처리 후 동일 모델로 재평가\n",
    "tfidf = TfidfVectorizer(stop_words = 'english')\n",
    "train_vec = tfidf.fit_transform(train_df.text)\n",
    "test_vec = tfidf.transform(test_df.text)\n",
    "\n",
    "clf_nb = MultinomialNB()\n",
    "scores = cross_val_score(clf_nb, train_vec, train_df['target'], cv = 5,\n",
    "                        scoring = 'f1')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74298551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62966031 0.60770328 0.65671642 0.66010598 0.73203492]\n"
     ]
    }
   ],
   "source": [
    "# 다른 vectorizer 활용\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cnt = CountVectorizer(stop_words = 'english')\n",
    "train_vec = cnt.fit_transform(train_df.text)\n",
    "test_vec = cnt.transform(test_df.text)\n",
    "\n",
    "clf_nb = MultinomialNB()\n",
    "scores = cross_val_score(clf_nb, train_vec, train_df['target'], cv = 5,\n",
    "                        scoring = 'f1')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ddf81f",
   "metadata": {},
   "source": [
    "count vectorzier에서 더 좋은 성능을 보이고 있는데, 이는 재난 문자에서는 공통적인 키워드가 들어가 있을 것이기에, 그 공통적인 키워드의 빈출을 살리기 위해서는 문서별 특이어를 잡아주는 TF-IDF보다는 공통어에 치중된 CountVectorzier의 성능이 더 좋을 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b63342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 확인을 위한 작업\n",
    "def submission(submission_file_path,model,test_vectors):\n",
    "    sample_submission = pd.read_csv(submission_file_path)\n",
    "    sample_submission[\"target\"] = model.predict(test_vectors)\n",
    "    sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20633015",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file_path = 'C:/Users/USER/Desktop/Data_Practices/Diaster_Tweeter/sample_submission.csv'\n",
    "test_vectors=test_vec\n",
    "model = clf_nb\n",
    "model.fit(train_vec, train_df['target'])\n",
    "submission(submission_file_path,clf_nb,test_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc18d3f7",
   "metadata": {},
   "source": [
    "위의 모델은 0.79436이라는 결과를 얻었다. 이제 조금 더 좋은 결과를 얻기 위해 개선 작업을 진행하고자 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734346d2",
   "metadata": {},
   "source": [
    "### 추가 변수 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0431a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>48</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>bbcmtd wholesale market ablaze</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>49</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Est. September 2012 - Bristol</td>\n",
       "      <td>we always try to bring the heavy metal rt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>50</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>africanbaze breaking newsnigeria flag set abla...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>52</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>cry out for more set me ablaze</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>53</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>London, UK</td>\n",
       "      <td>on plus side look at the sky last night it wa ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7578</th>\n",
       "      <td>10830</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>.</td>\n",
       "      <td>cameronhacker and i wrecked you both</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7579</th>\n",
       "      <td>10831</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Vancouver, Canada</td>\n",
       "      <td>three day off from work and theyve pretty much...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7580</th>\n",
       "      <td>10832</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>London</td>\n",
       "      <td>fx forex trading cramer igers word that wrecke...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7581</th>\n",
       "      <td>10833</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Lincoln</td>\n",
       "      <td>engineshed great atmosphere at the british lio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7582</th>\n",
       "      <td>10834</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>.</td>\n",
       "      <td>cramer igers word that wrecked disney stock cnbc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7552 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  keyword                       location  \\\n",
       "31       48   ablaze                     Birmingham   \n",
       "32       49   ablaze  Est. September 2012 - Bristol   \n",
       "33       50   ablaze                         AFRICA   \n",
       "34       52   ablaze               Philadelphia, PA   \n",
       "35       53   ablaze                     London, UK   \n",
       "...     ...      ...                            ...   \n",
       "7578  10830  wrecked                              .   \n",
       "7579  10831  wrecked              Vancouver, Canada   \n",
       "7580  10832  wrecked                        London    \n",
       "7581  10833  wrecked                        Lincoln   \n",
       "7582  10834  wrecked                              .   \n",
       "\n",
       "                                                   text  target  \n",
       "31                       bbcmtd wholesale market ablaze       1  \n",
       "32            we always try to bring the heavy metal rt       0  \n",
       "33    africanbaze breaking newsnigeria flag set abla...       1  \n",
       "34                       cry out for more set me ablaze       0  \n",
       "35    on plus side look at the sky last night it wa ...       0  \n",
       "...                                                 ...     ...  \n",
       "7578               cameronhacker and i wrecked you both       0  \n",
       "7579  three day off from work and theyve pretty much...       0  \n",
       "7580  fx forex trading cramer igers word that wrecke...       0  \n",
       "7581  engineshed great atmosphere at the british lio...       0  \n",
       "7582   cramer igers word that wrecked disney stock cnbc       0  \n",
       "\n",
       "[7552 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keyword가 있는 경우, 이미 keyword가 text에 포함되어 있음을 확인해 볼 수 있다.\n",
    "train_df.loc[train_df.keyword != '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1fecb0",
   "metadata": {},
   "source": [
    "### 다른 Word-Embedding : Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35c5527e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\user\\anaconda3\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (1.20.1)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (0.29.23)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (1.6.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995b29aa",
   "metadata": {},
   "source": [
    "Word2Vec을 위해서는 전체 코퍼스를 만들어야 하며,\n",
    "또한 이를 위해서는 앞에처럼 CountVectorizer 단계에서 stopwords를 지정하는 것이 아닌, 사전에 이를 지정해서 제외해야 하고, 이를 위해서는 토큰화된 상황으로 시작해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e52ee4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0a02ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def text_preprocess(text):\n",
    "    tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    lemmatizes = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    words = [w for w in lemmatizes if w not in stopwords.words('english')]    \n",
    "    combined_word = ' '.join(w for w in words)\n",
    "    \n",
    "    return combined_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7e6be7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.text = train_df.text.apply(lambda s : text_preprocess(s))\n",
    "test_df.text = test_df.text.apply(lambda s : text_preprocess(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bb96b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus 형성, 이 때 train, test 모두 해야 사이즈가 맞는다.\n",
    "corpus = pd.concat([train_df, test_df]).text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "318bb6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3b2de10",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_token = [nltk.word_tokenize(word) for word in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94522091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "word_vector_model = Word2Vec(sentences = corpus_token, vector_size = 150, \n",
    "                             window = 5, min_count = 5, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af96f96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('half', 0.9721816778182983),\n",
       " ('save', 0.9702348113059998),\n",
       " ('around', 0.9693827033042908),\n",
       " ('bush', 0.9689847826957703),\n",
       " ('danger', 0.9684366583824158),\n",
       " ('please', 0.9676346182823181),\n",
       " ('alarm', 0.9643679261207581),\n",
       " ('drought', 0.9634671211242676),\n",
       " ('whole', 0.9630415439605713),\n",
       " ('burn', 0.9625403881072998)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector_model.wv.most_similar('help')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8bda117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2360880, 3143040)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# corpus에 대한 훈련 실시\n",
    "word_vector_model.train(corpus_token, total_examples = len(corpus_token),\n",
    "                       epochs = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a522669b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
